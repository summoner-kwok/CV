- [背景知识](#背景知识)
  - [RGB-D相机](#rgb-d相机)
- [一，基于3DMM的三维人脸重建技术概述](#一基于3dmm的三维人脸重建技术概述)
  - [1.1，3D 人脸重建概述](#113d-人脸重建概述)
  - [1.2，初版 3DMM](#12初版-3dmm)
- [二，视觉SLAM算法基础概述](#二视觉slam算法基础概述)
  - [2.1，视觉里程计](#21视觉里程计)
  - [2.2，后端优化](#22后端优化)
  - [2.3，回环检测](#23回环检测)
  - [2.4，建图](#24建图)
- [三，三维点云语义分割和实例分割综述](#三三维点云语义分割和实例分割综述)
  - [3.1，三维数据的表示方法](#31三维数据的表示方法)
    - [3.1.1，点云定义](#311点云定义)
    - [3.1.2，点云的属性：](#312点云的属性)
    - [3.1.3，点云获取](#313点云获取)
    - [3.1.4，点云存储格式](#314点云存储格式)
    - [3.1.5，三维点云的多种表示方法](#315三维点云的多种表示方法)
  - [3.2，基于点云的分类和检测](#32基于点云的分类和检测)
  - [3.3，基于点云的语义分割](#33基于点云的语义分割)
    - [3.3.1，PointNet 网络](#331pointnet-网络)
- [四，参考资料](#四参考资料)

> 3D 视觉算法包括很多内容，此文仅当作入门了解些概念和知识概括。

## 背景知识
**3D图像描述有多种方法，常见的如下：**

* **点云**
* 网格（meshes）
* 基于视图的描述
* 深度图像（depth images）

### RGB-D相机
一般普通的相机拍出来的图像，其每个像素坐标（x, y）可以获得三种颜色属性（R, G, B）。但在 `RGB-D` 图像中，每个（x, y）坐标将对应于四个属性`（深度 D，R，G，B）`。

## 一，基于3DMM的三维人脸重建技术概述
### 1.1，3D 人脸重建概述
3D 人脸重建定义：从一张或多张2D图像中重建出人脸的3D模型。数学表达式：

$M = (S,T)$

其中 `S` 表示人脸 3D 坐标形状向量（shape-vector），`T` 表示对应点的纹理信息向量（texture-vector）。

3D 人脸重建算法分类：

![image](images/od05t0HHEArHqmf5MAOoz_IKMoc7otdHf3oHXicv5cc.png)

### 1.2，初版 3DMM
1999 年论文 《A Morphable Model For The Synthesis Of 3D Faces》提出三维形变模型（3DMM），三维形变模型建立在**三维人脸数据库**的基础上，以**人脸形状和人脸纹理**统计为约束，同时考虑到了人脸的姿态和光照因素的影响，因而生成的三维人脸模型精度高。每一个人脸模型都由相应的形状向量 $S_i$ 和 $T_i$组成，其定义如下：

$S_{newModel} = \bar{S} + \sum_{i=1}^{m-1} \alpha_{i} s_{i}$

$T_{newModel} = \bar{T} + \sum_{i=1}^{m-1} bata_{i} t_{i}$

其中 $\bar{S}$ 表示平均脸部形状模型，$s_i$表示 shape 的 PCA 部分（按照特征值降序排列的协方差阵的特征向量），$\alpha_i$表示对应形状系数；纹理模型符号定义类似。通过调整形状、纹理系数系数可生成不同的人脸 3D 模型。

## 二，视觉SLAM算法基础概述
>  SLAM问题的本质:对运动主体自身和周围环境空间不确定性的估计。为了解决SLAM问题，我们需要状 态估计理论，把定位和建图的不确定性表达出来，然后采用滤波器或非线性优化，估计状态的均值和不确定性(方差)。

`SLAM` 是Simultaneous Localization and Mapping的缩写，中文译作“同时定位与地图构建”。它是指搭载特定传感器（单目、双目、RGB-D相机、Lidar）的主体，在没有环境先验信息的情况下，**在运动过程中建立环境的模型**，同时估计自己的运动。如果这里的传感器主要为相机，那就称为“视觉SLAM”；如果传感器位激光，则为激光 SLAM，两者对比如下：

![image](images/skVDD0wo9TdzMWwEoTzz0i9-ojaVHx_b43uCRbt_mOg.png)

`SLAM` 主要解决**定位**和**地图构建**两个问题**。**视觉 SLAM 流程图如下：

![image](images/zxnT-RzBEc6SF6_lNSuZMRN0ZoKcBZBIx5XCXUzyRuI.png)

整个视觉 SLAM 流程包括以下几个步骤：

1. **传感器信息读取**。视觉 SLAM 中主要指摄像头图像数据读取与预处理。
2. **视觉里程计**（`Visual Odometry`, `VO`）。视觉里程计的任务是**估算相邻图像间相机的运动**，以及局部地图的样子。`VO` 又称为前端 (`Front End`)。
3. **后端优化**（`Optimization`）。后端接受不同时刻视觉里程计测量的相机位姿，以及回环检测的信息，对它们进行优化，得到全局一致的轨迹和地图。由于接在 `VO` 之后，又称为后端(`Back End`)。
4.  **回环检测 **（`Loop Closing`）。回环检测判断机器人是否到达过先前的位置。如果检测到回环，它会把信息提供给后端进行处理。
5.  **建图**（`Mapping`）。它根据估计的轨迹，建立与任务要求对应的地图。

### 2.1，视觉里程计
视觉里程计 `VO` 目的是**通过相邻帧间的图像估计相机运动**，并恢复场景的空间结构。其中为了定量地估计相机运动，必须先了解相机与空间点的几何关系。同时，仅通过视觉里程计来估计轨迹，将不可避免地出现累积漂移 (`Accumulating Drift`)，即每次估计都有误差的情况下，先前时刻的误差将会传递到下一个时刻，导致经过一段时间累积之后，估计的轨迹将不再准确，如下图所示。

![image](images/Mbb7-DFRD07EXmSOt8zoHyZ_bVtyPHRCkduDmGkUscA.png)

### 2.2，后端优化
概述性的说，后端优化主要目是为了处理 `SLAM` 过程中噪声的问题。后端优化要考虑的问题，就是如何从这些带有噪声的数据中估计整 个系统的状态，以及这个状态估计的不确定性有多大—这称为最大后 验概率估计(Maximum-a-Posteriori，MAP)。这里的状态既包括机器 人自身的轨迹，也包含地图。

前端与后端的关系：前端给后端提供待优化的数据，以及这些数据的初始值。后端只关心数据的优化过程，不关系这些数据来源于什么传感器。因此在视觉 `SLAM` 中，前端和计算机视觉研究领域更为相关，比如图像的特征提取与匹配等，后端则主要是**滤波与非线性优化算法**。

### 2.3，回环检测
回环检测（又称闭环检测 Loop Closure Detection），主要目的是为了**解决位置估计随时间漂移的问题**。

可以通过**图像相似性**来完成回环检测。在检测到回环之后，我们会把“A与B是同一个点”这样的信息告诉 后端优化算法。然后，后端根据这些新的信息，把轨迹和地图调整到符合回环检测结果的样子。这样，如果我们有充分而且正确的回环检测， 就可以消除累积误差，得到全局一致的轨迹和地图。

### 2.4，建图
建图(`Mapping`)是指构建地图的过程。这里的地图是对环境的描述，但这个描述并不是固定的，需要视SLAM 的应用而定。

![image](images/-C4C_ZobpoY4ZxyqJ1wukVuzmtdQhQBz-HIoOvs0RGA.png)

建图技术，根据用途的不同，可以分为稀疏重建和稠密重建，稀疏重建通常是重建一些图像特征点的三维坐标，**稀疏重建主要用于定位**。**稠密建图又称三维重建**，是对整个图像或图像中绝大部分像素进行重建，在导航、避障等方面起着举足轻重的作用。

![image](images/rBXnIbC9qHXn-qf_UTwhpYQLxFxXpGqWtAU1f-neY5g.png)

## 三，三维点云语义分割和实例分割综述
### 3.1，三维数据的表示方法
> 三维图像 = 普通的 RGB 三通道彩色图像 + Depth Map。

三维数据有四种表示方法，分别是 point cloud（点云），Mesh（网格），Voxel（体素）以及 Multi-View（多角度图片）。由此也衍生出了对应的三维数据语义和示例分割的算法，但主要是针对 point cloud 的算法越来越多。三维数据集有 ShapeNet、S3DIS、ModelNet40 等。

#### 3.1.1，点云定义
点云简单来说就是一堆三维点的集合，必须包括各个点的三维坐标信息，其他信息比如各个点的法向量、颜色、分类值、强度值、时间等均是可选。

点云在组成特点上分为两种，一种是有序点云，一种是无序点云。

* **有序点云**：一般由深度图还原的点云，有序点云按照图方阵一行一行的，从左上角到右下角排列，当然其中有一些无效点因为。有序点云按顺序排列，可以很容易的找到它的相邻点信息。有序点云在某些处理的时候还是很便利的，但是很多情况下是无法获取有序点云的。
* **无序点云**：无序点云就是其中的点的集合，点排列之间没有任何顺序，点的顺序交换后没有任何影响。是比较普遍的点云形式，有序点云也可看做无序点云来处理。

#### 3.1.2，点云的属性：
* 空间分辨率、点位精度、表面法向量等。
* 点云可以表达物体的空间轮廓和具体位置，我们能看到街道、房屋的形状，物体距离摄像机的距离也是可知的；其次，点云本身和视角无关，可以任意旋转，从不同角度和方向观察一个点云，而且不同的点云只要在同一个坐标系下就可以直接融合。

#### 3.1.3，点云获取
点云一般需要通过三维成像传感器获得，比如**双目相机、RGB-D相机和 LiDAR激光传感器**。

![image](images/8Ob5nzMaiIRZLVVECyi3oN-Z8XhfYzhAKhG5Xqc9Smk.png)

根据激光测量原理得到的点云，包括三维坐标（XYZ）和激光反射强度（Intensity），强度信息与目标的表面材质、粗糙度、入射角方向以及仪器的发射能量、激光波长有关。根据摄影测量原理得到的点云，包括三维坐标（XYZ）和颜色信息（RGB）。结合激光测量和摄影测量原理得到点云，包括三维坐标（`XYZ`）、激光反射强度（`Intensity`）和颜色信息（`RGB`）。

#### 3.1.4，点云存储格式
点云的文件格式可以有很多种，包括 .xyz，npy，ply，obj，off 等（mesh 可以通过泊松采样等方式转化成点云）。对于单个点云，如果你使用np.loadtxt得到的实际上就是一个维度为  的张量，num\_channels一般为 3，表示点云的三维坐标。

* **pts** 点云文件格式是最简便的点云格式，直接按 `XYZ` 顺序存储点云数据， 可以是整型或者浮点型。

![image](images/Pdm7Tzj8dFVqucN8eY6ndAejR3PnsKJ3b3U0NYQy5uA.png)

* **LAS** 是激光雷达数据（LiDAR），存储格式比 pts 复杂，旨在提供一种开放的格式标准，允许不同的硬件和软件提供商输出可互操作的统一格式。LAS 格式点云截图，其中 C：class(所属类)，F：flight(航线号)，T：time(GPS 时间)，I：intensity(回波强度)，R：return(第几次回波)，N：number of return(回波次数)，A：scan angle(扫描角)，RGB：red green blue(RGB 颜色值)。

![image](images/-pmmwckHPeNaLaysLDujqcFl5NHZB6vVD67sBKhxNOQ.png)

* **.xyz** 一种文本格式，前面 3 个数字表示点坐标，后面 3 个数字是点的法向量，数字间以空格分隔。

![image](images/2Hffp2Ah2aZCSjzTo7lZwi3pPaZ9MVB4szvGHwGUzxM.png)

* **.pcap** 是一种通用的数据流格式，现在流行的 Velodyne 公司出品的激光雷达默认采集数据文件格式。它是一种二进制文件

![image](images/b-KHrP8Z5byufrC3B4PHVrhsgdMiVhaUJ0tqKY59Lyk.png)

#### 3.1.5，三维点云的多种表示方法
三维点云除了原始点云表示还要网格 (Mesh) 表示和体素表示，如下图所示：

![image](images/dIFCzrqGsBuUHpjsPHHuTxo9qKmLudmIMx0U_FONin8.png)

### 3.2，基于点云的分类和检测
**背景：**相比于图像数据，点云不直接包含空间结构，因此点云的深度模型必须解决三个主要问题:

1. 如何从稀疏的点云找到高信息密度的表示。
2. 如何构建一个网络满足必要的限制如 size-variance 和 permutation-invariance。
3. 如何以较低的时间和计算资源消耗处理大量数据。

对点云的分类通常称为三维形状分类。与图像分类模型相似，三维形状分类模型通常是先通过聚合编码器生成全局嵌入，然后将嵌入通过几个完全连通的层来获得最终结果。基于点云聚合方法，分类模型大致可分为两类: **基于投影的方法**和**基于点的方法。**

### 3.3，基于点云的语义分割
基于点云的语义分割方法大致可分为**基于投影的方法和基于点的方法。**

#### 3.3.1，PointNet 网络
PointNet 是第一个可以直接处理原始三维点云的深度神经网络，简单来说 `PointNet` 所作的事情就是对点云做特征学习，并将学习到的特征去做不同的应用：分类（shape-wise feature）、分割（point-wise feature）等。

无论是分类还是分割，本质上都还是分类任务，只是粒度不同罢了。因此损失函数 `loss` 一定有有监督分类任务中常用的交叉熵 loss，另外 loss 还有之前 `alignment network`（用于实现网络对于仿射变换、刚体变换等变换的无关性）的约束 loss，也就是上面的 `mat_diff_loss` 。

PointNet 网络结构如下所示：

![image](images/J0OEbxtK5vPs73_KRBIRLbSue5Zu1VBkYCCZkN0zOSc.png)

其大致的运算流程如下（来自[【3D视觉】PointNet和PointNet++](https://zhuanlan.zhihu.com/p/336496973)）：

1. 输入为一帧的全部点云数据的集合，表示为一个 nx3 的 2d tensor，其中 n 代表点云数量，3 对应 xyz 坐标。
2. 输入数据先通过和一个 `T-Net`**学习到的转换矩阵**相乘来对齐，保证了模型的对特定空间转换的不变性。
3. 通过多次 mlp 对各点云数据进行特征提取后，再用一个 T-Net 对特征进行对齐。
4. 在特征的各个维度上执行 **maxpooling **操作来得到最终的**全局特征**。
5. 对分类任务，将全局特征通过 mlp 来预测最后的分类分数。
6. 对分割任务，将全局特征和之前学习到的各点云的局部特征进行串联，再通过 mlp 得到每个数据点的分类结果。

分割任务**针对于每一个点做分类**，在下面的图中，把全局的特征复制成 `n` 份然后与之前的 `64` 维特征进行拼接，然后接着做一个 `mlp`，最后的输出 `nxm` 就是每一个点的分类结果。

![image](images/5zz0t-vreeXLIjFuy3d0_DlB0gCBxn1zXlFUBcZxt_E.png)

## 四，参考资料
1. [细嚼慢咽读论文：PointNet论文及代码详细解析](https://zhuanlan.zhihu.com/p/264627148?utm_source=wechat_session&utm_medium=social&utm_oi=1135649954939883520&utm_campaign=shareopn)
2. [3D点云基础知识](https://zhuanlan.zhihu.com/p/344635951)
3. [【3D视觉】PointNet和PointNet++](https://zhuanlan.zhihu.com/p/336496973)
4. [点云+深度学习的开山之作–Pointnet](https://www.aimlab.top/1694.html)